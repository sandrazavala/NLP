{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "\n",
    "In this first homework, we'll introduce some basic operations of working with static word embeddings (calculating the cosine similarity between two vectors, and finding the nearest neighbors in vector space among a set of embeddings), and then use those basic operations to learn something interesting about the datasets those vectors were estimated from: measuring the orientation of words along different semantic axes (e.g., the degree to which a word like \"doctor\" is associated with men or women in the underlying dataset); and measuring the *change* in meaning of a word across domains, including time (e.g., \"cabinet\", \"awesome\", etc.).\n",
    "\n",
    "In this notebook, feel free to use [numpy](https://numpy.org/doc/stable/user/quickstart.html) for vector operations, but **do not import any other libraries** outside of those already provided (e.g., do not import pandas).  Many of the homeworks that we'll have later in this course will use vector and matrix operations in libraries like pytorch that are very similar to numpy, so it's worth getting some exposure to numpy now.\n",
    "\n",
    "## Deliverables:\n",
    "\n",
    "There are two different deliverables, each to be submitted to a different Gradescope assignment.\n",
    "\n",
    "1. Submit to GradeScope **HW1 california_nearest_neighbors_50.txt**: california_nearest_neighbors_50.txt. \n",
    "2. Submit to GradeScope **HW1 code**: HW1.ipynb (this notebook)\n",
    "\n",
    "(Please don't alter either of these file names.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vectors(filename):\n",
    "    vocab=[]\n",
    "    vocab_map={}\n",
    "    embeddings=[]\n",
    "    with(open(filename, encoding=\"utf-8\")) as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            word=cols[0]\n",
    "            embedding=cols[1:]\n",
    "\n",
    "            embeddings.append(embedding)\n",
    "            vocab.append(word)\n",
    "            vocab_map[word]=idx\n",
    "    \n",
    "    return vocab, vocab_map, np.array(embeddings, dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab, glove_vocab_map, glove_embeddings=read_vectors(\"glove.6B.100d.100K.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** As we saw in class, one of the most common ways of measuring the similarity of two vectors in NLP is cosine similarity. Write a function to calculate the cosine similarity between any two **numpy** vectors (as with the word embeddings above); this function should return a single real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \n",
    "    # Fill in code here\n",
    "    return np.dot(vec1,vec2) / ( (np.dot(vec1,vec1) **.5) * (np.dot(vec2,vec2) ** .5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** A core operation on word embeddings is to find the $k$-nearest neighbors to a word: e.g., given a target word like \"california\" and $k=10$, finding the 10 words in your vocabulary whose embeddings have the highest cosine similarity to the embedding for \"california\".  Write a function that that does just that, given an input set of embeddings, `vocab_map` from the `read_vectors` function, and a query term.  Your function should return a list of the $k$-nearest neighbors in order from most similar to least, and only those $k$ words (e.g., if $k$=2 and `query_term` = \"california\", you should return `[\"texas\", \"florida\"]`.  Do not include the query term itself among the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(embeddings, vocab_map, query_term, k=10):\n",
    "    nearest_neighbors=[]\n",
    "    similarities = []\n",
    "    # Fill in code here\n",
    "    # vocab_map = context\n",
    "    #ouputs index for given query term\n",
    "    var = next((i for i, item in enumerate(vocab_map) if item == query_term), None)\n",
    "    \n",
    "    for i in embeddings:\n",
    "            #compare query_term embeddings to all embeddings\n",
    "            x = cosine_similarity(embeddings[var], i)\n",
    "            #append to list\n",
    "            similarities.append(x)\n",
    "            #returns indices of cosine sim vals\n",
    "    #sort nearest neighbors\n",
    "    similarities = np.argsort(similarities)\n",
    "    #filter to top 10 - excluding california\n",
    "    similarities = similarities[-(k+1):][::-1][1:(k+1)]\n",
    "    #use list of indices to get\n",
    "    for i in similarities:\n",
    "        key_list = list(vocab_map.keys())\n",
    "        val_list = list(vocab_map.values())\n",
    "        nearest_neighbors.append(key_list[i])\n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tvehicle\n",
      "1\ttruck\n",
      "2\tcars\n",
      "3\tdriver\n",
      "4\tdriving\n",
      "5\tmotorcycle\n",
      "6\tvehicles\n",
      "7\tparked\n",
      "8\tbus\n",
      "9\ttaxi\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors=find_nearest_neighbors(glove_embeddings, glove_vocab_map, \"car\", k=10)\n",
    "for idx, k in enumerate(nearest_neighbors):\n",
    "    print(\"%s\\t%s\" % (idx, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to use your find_nearest_neighbors function above to find the 50 nearest neighbors\n",
    "# to the word \"california\".  This cell writes that output to the file california_nearest_neighbors_50.txt,\n",
    "# which you will upload to GradeScope as a deliverable.\n",
    "\n",
    "# DO NOT CHANGE THIS BLOCK\n",
    "nearest_neighbors=find_nearest_neighbors(glove_embeddings, glove_vocab_map, \"california\", k=50)\n",
    "with open(\"california_nearest_neighbors_50.txt\", \"w\") as out:\n",
    "    for idx, k in enumerate(nearest_neighbors):\n",
    "        out.write(\"%s\\t%s\\n\" % (idx, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemAxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are the foundation of many of the NLP models we'll use in this class, and provide a representation of input words for a range of different tasks.  But we can also interrogate the representations themselves to examine a number of different questions.  First, let's consider the question of how to measure the orientation of word representations along specific semantic axes: e.g., for an axis defined by the endpoints \"happy\" and \"sad\", where do word embeddings estimated from a specific dataset situate a word like \"smile\"?  As we've seen in class, this gives us a mechanism for interrogating bias: if we define an axis by the endpoints \"man\" and \"woman\", for example, where do we see words like \"doctor\" and \"nurse\" appearing along this spectrum? (For similar word in this vein, see [Bolukbasi et al. 2016](https://arxiv.org/pdf/1607.06520.pdf), [Blodgett et al. 2020](https://aclanthology.org/2020.acl-main.485.pdf).)\n",
    "\n",
    "[SemAxis](https://arxiv.org/pdf/1806.05521.pdf) is one such method, where the axis endpoints are defined not by single words, but by sets of words (e.g., \"happy\", \"cheerful\", \"ecstatic\"). Given a set of word embeddings for one category $S^+ = \\{v_1^+, \\ldots v_n^+\\}$ and embeddings for a contrasting category $S^- = \\{v_1^-, \\ldots v_n^-\\}$ that both define the endpoints of the axis, SemAxis outputs a single real-value score for an input word $w$ with word representation $v_w$:\n",
    "\n",
    "$$\n",
    "score(w)_{\\mathbf{V_\\textrm{axis}}} = \\textrm{cos}(v_w, \\mathbf{V}_\\textrm{axis})\n",
    "$$\n",
    "\n",
    "Where: \n",
    "$$\n",
    "\\mathbf{V}^+ = {1 \\over n} \\sum_1^n v_i^+\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}^- = {1 \\over m} \\sum_1^m v_i^-\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_{\\textrm{axis}} = \\mathbf{V}^+ - \\mathbf{V}^-\n",
    "$$\n",
    "\n",
    "Let's investigate how we can use the methods above to situate words along axes you define.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semaxis_score(vectors, vocab_map, positive_terms=None, negative_terms=None, target_word=None):\n",
    "    \n",
    "    positive_vecs=[]\n",
    "    negative_vecs=[]\n",
    "    \n",
    "    for term in positive_terms:\n",
    "        positive_vecs.append(vectors[vocab_map[term]])\n",
    "    \n",
    "    for term in negative_terms:\n",
    "        negative_vecs.append(vectors[vocab_map[term]])\n",
    "        \n",
    "    v_plus=np.mean(positive_vecs, axis=0)\n",
    "    v_neg=np.mean(negative_vecs, axis=0)\n",
    "    \n",
    "    v_axis=v_plus-v_neg\n",
    "    \n",
    "    target_vec=vectors[vocab_map[target_word]]\n",
    "    \n",
    "    score=cosine_similarity(target_vec, v_axis)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_list_of_targets(vectors, vocab_map, positive_terms=None, negative_terms=None, target_words=None):\n",
    "    scores=[]\n",
    "    for target in target_words:\n",
    "        scores.append((get_semaxis_score(vectors, vocab_map, positive_terms, negative_terms, target), target))\n",
    "\n",
    "    for k,v in reversed(sorted(scores)):\n",
    "        print(\"%.3f\\t%s\" % (k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=[\"doctor\", \"nurse\", \"actor\", \"actress\", \"mechanic\", \"librarian\", \"architect\", \"magician\", \"cook\", \"chef\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.342\tactress\n",
      "0.294\tnurse\n",
      "0.219\tlibrarian\n",
      "0.106\tdoctor\n",
      "0.024\tactor\n",
      "0.003\tchef\n",
      "-0.019\tcook\n",
      "-0.075\tarchitect\n",
      "-0.153\tmagician\n",
      "-0.194\tmechanic\n"
     ]
    }
   ],
   "source": [
    "score_list_of_targets(glove_embeddings, glove_vocab_map, positive_terms=[\"woman\", \"women\"], negative_terms=[\"man\", \"men\"], target_words=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** Define your own concept axis by selecting a set of positive and negative terms (as we did for {woman, women}, {man, men} above) and illustrate its utility by scoring a set of 10 target terms.  You may use any axis and target terms that you think can yield an interesting insight; for examples of axes other related work has explored), see [Kozlowski et al. 2019](https://journals.sagepub.com/doi/pdf/10.1177/0003122419877135)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.122\tdelicate\n",
      "0.090\tqualified\n",
      "0.032\tpoor\n",
      "0.005\tbrave\n",
      "-0.006\tsmart\n",
      "-0.015\tstrong\n",
      "-0.031\tweak\n",
      "-0.060\trich\n"
     ]
    }
   ],
   "source": [
    "positive_terms=[\"woman\", \"women\",\"female\",\"she\"]\n",
    "negative_terms=[\"man\", \"men\",\"male\",\"he\"]\n",
    "targets=[\"qualified\",\"poor\",\"rich\",\"strong\",\"weak\",\"smart\",\"delicate\",\"brave\"]\n",
    "\n",
    "score_list_of_targets(glove_embeddings, glove_vocab_map, positive_terms=positive_terms, negative_terms=negative_terms, target_words=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word sense change\n",
    "\n",
    "---\n",
    "\n",
    "Lots of work in NLP has used word embeddings to examine how word meanings have changed over time (e.g., [Hamilton et al. 2016](https://arxiv.org/pdf/1606.02821.pdf), [Garg et al. 2018](https://www.pnas.org/content/115/16/E3635.short), [Kulkarni et al. 2014](https://arxiv.org/pdf/1411.3315.pdf)).  We can examine this here by looking at word embeddings trained on datasets written at different times: GloVe vectors trained on contemporary text (including Wikipedia and the general web), and vectors trained on literary texts from Project Gutenberg (mainly written before 1925).  We can't directly compare two vectors estimated in separate training procedures (since the embedding spaces are not equivalent), but we can compare the overlap in their nearest neighbors to get a sense of the degree of their change across these different domains.\n",
    "\n",
    "(**There is no deliverable here**, but feel free to play around to see how words have changed their meaning over time using nearest neighbor associations as the means of doing so. What words do you think have changed in meaning over the past 100 years?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_vocab, gutenberg_vocab_map, gutenberg_embeddings=read_vectors(\"gutenberg.200.vectors.50K.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_nearest_neighbors=find_nearest_neighbors(glove_embeddings, glove_vocab_map, \"cabinet\", k=10)\n",
    "print(glove_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_nearest_neighbors=find_nearest_neighbors(gutenberg_embeddings, gutenberg_vocab_map, \"cabinet\", k=10)\n",
    "print(gutenberg_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nn_overlap(term):\n",
    "    glove_nearest_neighbors=find_nearest_neighbors(glove_embeddings, glove_vocab_map, term, k=10)\n",
    "    print(\"GloVe:\", glove_nearest_neighbors)\n",
    "    gutenberg_nearest_neighbors=find_nearest_neighbors(gutenberg_embeddings, gutenberg_vocab_map, term, k=10)\n",
    "    print(\"Gutenberg:\", gutenberg_nearest_neighbors)\n",
    "    overlap=set(glove_nearest_neighbors).intersection(set(gutenberg_nearest_neighbors))\n",
    "    print(overlap)\n",
    "    print(len(overlap)/len(glove_nearest_neighbors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_nn_overlap(\"cabinet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
